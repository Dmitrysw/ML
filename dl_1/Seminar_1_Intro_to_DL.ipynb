{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Введение. Полносвязные слои. Функции активации (ноутбук)\n",
    "\n",
    "> Начнем осваивать библиотеку `PyTorch`. Познакомимся с нейронными сетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## План ноутбука\n",
    "\n",
    "1. Установка `PyTorch`\n",
    "1. Введение в `PyTorch`\n",
    "1. Полносвязные слои и функции активации в `PyTorch`\n",
    "1. Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Установка `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы будем использовать библиотеку для глубинного обучения `PyTorch`, ее можно не устанавливать, будем пользоваться сайтом [kaggle.com](kaggle.com) для обучения в облаке (или с учителем?). \n",
    "\n",
    "Чтобы установить `PyTorch` локально себе на компьютер нужно ответить на два вопроса - какая у вас операционная система и есть ли у вас дискретная видеокарта (GPU) и если есть, то какого производителя. В зависимости от ваших ответов мы получаем три варианта по операционной системе - Linux, Mac и Windows; три варианта по дискретной видеокарте - нет видеокарты (доступен только центральный процессор CPU), есть видеокарта от Nvidia или есть видеокарта от AMD (это производитель именно чипа, конечный вендор может быть другой, например, ASUS, MSI, Palit). Работа с PyTorch с видеокартой от AMD это экзотика, которая выходит за рамки нашего курса, поэтому рассмотрим только варианты *нет видеокарты*/*есть видеокарта от Nvidia*.\n",
    "\n",
    "\n",
    "Выберите на [сайте](https://pytorch.org/get-started/locally/) подходящие вам варианты операционной системы/видеокарты и скопируйте команду для установки. Разберем подробно самые популярные варианты установки:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Linux ([поддерживаемые дистрибутивы](https://pytorch.org/get-started/locally/#supported-linux-distributions))\n",
    "\n",
    "На линуксе будет работать поддержка `PyTorch` в любой конфигурации, что у вас нет видеокарты, что есть от Nvidia, что от AMD. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка в Windows\n",
    "\n",
    "На винде будет работать поддержка `PyTorch` только для видеокарт от Nvidia и без видеокарт вообще. \n",
    "\n",
    "Пререквизит для работы с видеокартой от Nvidia - нужно поставить CUDA, это инструмент от компании Nvidia, который позволяет ускорять вычисления на их же ГПУ. Чтобы поставить себе на машину все правильно воспользуйтесь этим [гайдом](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html) от Nvidia.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` для тех, у кого нет видеокарты.\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116` для тех, у кого есть видеокарта (либо другой `--extra-index-url`, смотрите на сайте PyTorch, в зависимости от версии CUDA).\n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch` для тех, у кого нет видеокарты.\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge` для тех, у кого есть видеокарта (либо немного другая команда, в зависимости от версии CUDA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Установка на Mac\n",
    "\n",
    "На маках есть пока что поддержка `PyTorch` только центрального процессора, чуть позже появится поддержка ускорения на чипах M1, M2, M1 Pro и так далее.\n",
    "\n",
    " - **pip**\n",
    "\n",
    "`pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu` \n",
    "\n",
    " - **conda**\n",
    "\n",
    "`conda install pytorch torchvision torchaudio cpuonly -c pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Введение в `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Тензоры\n",
    "\n",
    "Тензоры — это специализированная структура данных, по сути это массивы и матрицы. Тензоры очень похожи на массивы в numpy, так что, если у вас хорошо с numpy, то разобраться в PyTorch тензорах будет очень просто. В PyTorch мы используем тензоры для кодирования входных и выходных данных модели, а также параметров модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Создание тензоров\n",
    "\n",
    "Тензор можно создать напрямую из каких-то данных - нам подходят все списки с числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 2, 3, 4])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [1, 2, 3, 4]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[1, 2], [3, 4], [5, 6]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1],\n         [2]],\n\n        [[3],\n         [4]],\n\n        [[5],\n         [6]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "На самом деле про \"все\" списки с числами - обман. Если у вашего списка есть какой-то уровень вложенности, то должны совпадать размерности у всех вложенных списков (подробнее про размерности поговорим позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 2 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m some_other_data \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m], [\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m], [\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m7\u001B[39m]]\n\u001B[1;32m----> 2\u001B[0m some_other_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msome_other_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m some_other_tensor\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 2 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "some_other_data = [[1, 2], [3, 4], [5, 6, 7]]\n",
    "some_other_tensor = torch.tensor(some_other_data)\n",
    "\n",
    "some_other_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Также тензоры можно создавать из numpy массивов и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[1],\n        [2]],\n\n       [[3],\n        [4]],\n\n       [[5],\n        [6]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_numpy_array = np.array(some_data)\n",
    "\n",
    "some_numpy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1],\n         [2]],\n\n        [[3],\n         [4]],\n\n        [[5],\n         [6]]], dtype=torch.int32)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor_from_numpy = torch.from_numpy(some_numpy_array)\n",
    "\n",
    "some_tensor_from_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "При этом если мы создаем тензор из numpy массива с помощью `torch.from_numpy`, то они делят между собой память, где лежат их данные и, соответственно, при изменении тензора меняется numpy массив и наоборот:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=torch.float64))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones(10)\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10)\n",
    "y = x.numpy()\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x += 1\n",
    "\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Можем создать тензор со случайными или константными значениями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3685, 0.6298, 0.6781],\n",
       "         [0.8752, 0.1232, 0.8770]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[1.4013e-45, 0.0000e+00, 3.8344e-10],\n",
       "         [4.5678e-41, 0.0000e+00, 0.0000e+00]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "\n",
    "random_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "empty_tensor = torch.empty(shape)\n",
    "\n",
    "random_tensor, ones_tensor, zeros_tensor, empty_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь поговорим про размерности подробнее.\n",
    "\n",
    "У тензора есть какой-то размер, какая форма. Первое с чем нужно определиться, какой **размерности** тензор - количество осей у него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9474, 0.1858, 0.8743, 0.5387, 0.2358, 0.7229, 0.9503, 0.6604, 0.6113,\n        0.9937])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (10)  # одна ось (вектор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.9784, 0.4202, 0.4202],\n        [0.0620, 0.8795, 0.6330]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (2, 3)  # две оси (матрица)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.3244, 0.8238, 0.8014],\n         [0.9674, 0.9790, 0.0881]],\n\n        [[0.1361, 0.8744, 0.5116],\n         [0.3505, 0.5862, 0.2421]],\n\n        [[0.8002, 0.0689, 0.9985],\n         [0.8403, 0.7834, 0.4438]]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (3, 2, 3)  # три оси (и больше - тензор)\n",
    "\n",
    "tensor = torch.rand(shape)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Тензор с размерностью 1 - это просто вектор, список чисел.\n",
    "\n",
    "Тензор с размерностью 2 - это просто матрица, то есть список списков чисел.\n",
    "\n",
    "Тензор с размерностью 3 и больше - это тензор, то есть список списков списков ... чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Получить доступ к размеру уже созданного тензора - метод `.shape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "some_data = [[[1], [2]], [[3], [4]], [[5], [6]]]\n",
    "some_tensor = torch.tensor(some_data)\n",
    "\n",
    "print(some_tensor)\n",
    "print(some_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В лекции мы говорили про изображения, давайте сделаем тензор, который будет нам имитировать изображение - сделаем его размер `(c, h, w)`, где `h` и `w` это его высота и ширина, а `c` - число каналов в цветовом пространстве (в черно-белом 1, в RGB 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[6.0908e-01, 8.2688e-01, 3.6211e-01, 5.0674e-01, 3.8604e-01,\n          6.8298e-01, 7.2011e-01, 6.3922e-01, 6.5938e-01, 5.3771e-02,\n          8.5798e-01, 2.2827e-01, 1.7249e-01, 9.4449e-01, 1.9916e-01,\n          5.5284e-01],\n         [8.1752e-01, 5.8065e-01, 3.1039e-01, 4.7719e-01, 9.8907e-01,\n          2.6862e-01, 4.8310e-01, 3.7077e-01, 1.3279e-01, 5.7344e-01,\n          3.3118e-01, 5.0580e-02, 5.7516e-01, 1.1368e-01, 5.7049e-01,\n          3.2255e-02],\n         [3.3409e-01, 9.8763e-01, 7.5251e-01, 8.3417e-01, 2.5541e-01,\n          7.9979e-01, 6.8651e-01, 8.2326e-01, 9.4956e-01, 9.1910e-01,\n          5.4953e-01, 7.5521e-01, 8.1268e-01, 9.4345e-01, 3.2955e-01,\n          9.8029e-01],\n         [6.3454e-01, 7.1590e-01, 1.4699e-01, 4.9875e-01, 6.6452e-01,\n          9.8619e-01, 1.7869e-01, 9.8631e-01, 7.7994e-01, 6.4123e-01,\n          8.7713e-01, 3.7647e-01, 9.0813e-01, 1.9918e-01, 3.7610e-01,\n          4.1445e-01],\n         [2.3531e-01, 1.8936e-01, 2.1949e-02, 1.6616e-02, 1.0486e-01,\n          8.5149e-01, 8.0885e-01, 1.4436e-01, 8.7623e-01, 9.2560e-01,\n          1.6186e-02, 4.6024e-01, 6.6257e-02, 9.3920e-01, 1.3933e-01,\n          5.5780e-01],\n         [3.4164e-01, 2.8029e-01, 4.7403e-01, 4.2889e-01, 3.5084e-01,\n          1.4187e-01, 4.4056e-01, 5.7033e-01, 4.9778e-01, 8.1038e-01,\n          5.7606e-01, 4.8116e-01, 2.2426e-01, 8.7398e-01, 6.2634e-01,\n          3.8769e-01],\n         [8.7170e-01, 6.6032e-01, 4.8786e-03, 6.0990e-02, 1.2778e-01,\n          2.9638e-01, 2.0806e-01, 1.3343e-01, 2.5414e-02, 4.1106e-01,\n          8.5384e-01, 8.7712e-01, 1.7310e-02, 6.8996e-02, 3.2370e-02,\n          9.8035e-01],\n         [9.3215e-01, 5.9434e-01, 8.0573e-01, 3.1583e-01, 3.4815e-01,\n          8.9839e-01, 2.7450e-02, 9.3439e-01, 7.4615e-01, 1.6936e-01,\n          3.9897e-01, 9.2557e-01, 6.1975e-01, 6.9744e-01, 3.9812e-01,\n          7.6628e-01],\n         [4.6205e-01, 2.5668e-01, 3.5637e-01, 9.7589e-01, 9.7539e-01,\n          6.9650e-02, 4.2055e-01, 5.2815e-01, 4.1325e-01, 6.0469e-01,\n          9.5184e-01, 5.7837e-01, 6.8111e-01, 3.9773e-01, 4.7398e-01,\n          9.6995e-01]],\n\n        [[8.9349e-02, 7.1366e-01, 3.8643e-01, 9.4546e-01, 3.3062e-01,\n          7.4520e-03, 7.8092e-03, 7.6223e-01, 8.8274e-01, 6.8050e-01,\n          4.3685e-01, 1.0996e-01, 4.7560e-01, 9.3365e-02, 1.7165e-01,\n          1.2536e-01],\n         [9.9105e-01, 9.9123e-01, 5.2001e-01, 1.8239e-01, 7.0742e-01,\n          6.7226e-01, 4.2759e-01, 5.6319e-01, 4.2051e-01, 6.9120e-02,\n          6.6342e-01, 9.5281e-01, 4.3755e-01, 9.5999e-01, 7.5591e-01,\n          8.3064e-01],\n         [8.3476e-01, 7.4947e-01, 5.9000e-01, 2.1103e-01, 5.0039e-01,\n          2.7622e-01, 6.3914e-01, 4.6453e-01, 4.8232e-04, 4.8678e-01,\n          7.0276e-01, 1.2048e-01, 3.0000e-01, 3.7730e-01, 6.1310e-03,\n          5.1246e-01],\n         [9.7980e-01, 4.6687e-01, 2.7049e-01, 4.2934e-01, 7.8575e-01,\n          3.0175e-01, 1.4535e-01, 1.4961e-01, 4.6824e-01, 1.4982e-01,\n          3.5293e-01, 5.0061e-01, 4.2234e-02, 5.3242e-01, 3.9917e-01,\n          7.9273e-01],\n         [5.7077e-01, 9.3541e-01, 9.7752e-03, 2.6741e-01, 5.3530e-01,\n          9.5146e-01, 7.4037e-01, 7.2323e-01, 9.4544e-01, 8.1587e-01,\n          9.6551e-01, 5.8702e-01, 4.4200e-01, 9.1728e-01, 8.0925e-01,\n          7.1473e-01],\n         [6.1602e-01, 6.0771e-01, 5.2970e-01, 1.9444e-01, 7.7686e-01,\n          6.8607e-01, 8.7017e-01, 9.3810e-01, 2.0961e-01, 8.0898e-02,\n          8.2443e-01, 5.1144e-01, 4.7545e-01, 2.8544e-01, 3.9306e-01,\n          6.0405e-01],\n         [3.9088e-01, 7.0508e-01, 6.8278e-01, 2.4696e-01, 5.5319e-01,\n          5.2963e-01, 6.5997e-01, 3.6849e-01, 2.3453e-02, 3.3473e-01,\n          8.8468e-01, 7.7119e-01, 5.2400e-01, 1.6325e-01, 8.8217e-01,\n          8.3910e-01],\n         [6.4840e-01, 4.8592e-01, 4.2074e-01, 7.1069e-01, 9.9769e-02,\n          1.2881e-01, 7.6820e-01, 3.0431e-01, 6.0925e-01, 5.2847e-01,\n          3.3189e-01, 9.5496e-01, 9.5380e-01, 5.0207e-01, 4.0477e-01,\n          4.7102e-01],\n         [6.4067e-01, 1.5120e-01, 4.2124e-01, 9.2751e-01, 3.4550e-01,\n          6.8743e-02, 4.4667e-01, 1.9811e-01, 1.8481e-02, 9.1619e-01,\n          5.7966e-03, 8.0385e-01, 7.0425e-01, 3.7842e-01, 9.6244e-01,\n          3.7289e-02]],\n\n        [[5.1059e-01, 6.5691e-02, 6.1693e-01, 5.0997e-01, 9.0126e-01,\n          7.0149e-02, 4.5577e-01, 6.7399e-01, 7.8183e-01, 5.1805e-01,\n          8.2415e-01, 7.2963e-01, 9.3829e-01, 6.2880e-01, 3.0464e-01,\n          2.9609e-01],\n         [7.1180e-04, 8.5032e-01, 8.8195e-01, 6.2198e-01, 6.6600e-02,\n          3.5440e-01, 3.9870e-01, 7.1794e-01, 2.5072e-01, 5.3895e-02,\n          3.7280e-01, 1.2637e-02, 4.2150e-01, 4.4096e-02, 4.0387e-01,\n          4.6432e-01],\n         [6.2618e-02, 1.8862e-01, 4.7754e-01, 4.4415e-01, 2.5383e-01,\n          3.7587e-01, 9.1606e-01, 7.8984e-01, 2.6505e-01, 2.6363e-01,\n          2.5995e-01, 5.7268e-01, 3.8814e-01, 7.7290e-01, 9.0941e-01,\n          4.2257e-01],\n         [1.5339e-01, 4.7617e-01, 1.4643e-01, 9.4751e-01, 8.5553e-01,\n          5.2902e-01, 6.3557e-01, 1.0439e-01, 2.8087e-01, 5.7533e-01,\n          4.0643e-01, 9.6533e-02, 2.0074e-01, 6.1255e-01, 6.1688e-01,\n          6.7355e-01],\n         [2.7356e-01, 3.0159e-01, 6.9753e-01, 4.0380e-01, 1.4077e-01,\n          5.2356e-01, 6.5850e-01, 8.6789e-01, 1.9443e-01, 9.3979e-01,\n          7.8556e-01, 4.0803e-01, 4.9676e-01, 1.2908e-01, 4.9485e-01,\n          1.7348e-01],\n         [6.6231e-01, 1.7308e-01, 6.4689e-01, 9.7923e-01, 1.0211e-01,\n          1.6433e-01, 3.6372e-01, 1.2851e-01, 4.7264e-01, 9.6455e-01,\n          2.2765e-01, 3.8615e-01, 1.0712e-01, 8.6763e-02, 4.8813e-01,\n          8.9692e-01],\n         [5.7527e-01, 2.6399e-01, 2.5632e-01, 3.7566e-01, 5.2084e-01,\n          4.7470e-01, 9.7930e-01, 3.5166e-01, 9.1898e-01, 8.4318e-01,\n          4.9218e-01, 8.6635e-01, 9.6121e-01, 9.9477e-01, 7.0478e-01,\n          7.4428e-01],\n         [2.0494e-01, 1.6206e-01, 2.2478e-01, 3.0600e-01, 4.9878e-01,\n          2.2007e-01, 2.4453e-02, 6.6675e-01, 9.0109e-01, 7.9572e-01,\n          3.4442e-01, 9.6500e-01, 9.4492e-01, 8.4520e-01, 1.4979e-01,\n          4.5379e-01],\n         [2.1712e-01, 9.0614e-02, 5.5350e-01, 2.8294e-01, 6.7821e-02,\n          1.8451e-01, 6.1734e-01, 5.0501e-01, 6.0500e-01, 4.8678e-01,\n          7.7853e-01, 1.0100e-01, 9.3932e-01, 9.6433e-01, 3.2737e-01,\n          5.0535e-01]]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 9\n",
    "w = 16\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 9, 16])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Можем попробовать поменять размер тензора, например, [вытянуть его в вектор](https://pytorch.org/docs/stable/generated/torch.ravel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([6.0908e-01, 8.2688e-01, 3.6211e-01, 5.0674e-01, 3.8604e-01, 6.8298e-01,\n        7.2011e-01, 6.3922e-01, 6.5938e-01, 5.3771e-02, 8.5798e-01, 2.2827e-01,\n        1.7249e-01, 9.4449e-01, 1.9916e-01, 5.5284e-01, 8.1752e-01, 5.8065e-01,\n        3.1039e-01, 4.7719e-01, 9.8907e-01, 2.6862e-01, 4.8310e-01, 3.7077e-01,\n        1.3279e-01, 5.7344e-01, 3.3118e-01, 5.0580e-02, 5.7516e-01, 1.1368e-01,\n        5.7049e-01, 3.2255e-02, 3.3409e-01, 9.8763e-01, 7.5251e-01, 8.3417e-01,\n        2.5541e-01, 7.9979e-01, 6.8651e-01, 8.2326e-01, 9.4956e-01, 9.1910e-01,\n        5.4953e-01, 7.5521e-01, 8.1268e-01, 9.4345e-01, 3.2955e-01, 9.8029e-01,\n        6.3454e-01, 7.1590e-01, 1.4699e-01, 4.9875e-01, 6.6452e-01, 9.8619e-01,\n        1.7869e-01, 9.8631e-01, 7.7994e-01, 6.4123e-01, 8.7713e-01, 3.7647e-01,\n        9.0813e-01, 1.9918e-01, 3.7610e-01, 4.1445e-01, 2.3531e-01, 1.8936e-01,\n        2.1949e-02, 1.6616e-02, 1.0486e-01, 8.5149e-01, 8.0885e-01, 1.4436e-01,\n        8.7623e-01, 9.2560e-01, 1.6186e-02, 4.6024e-01, 6.6257e-02, 9.3920e-01,\n        1.3933e-01, 5.5780e-01, 3.4164e-01, 2.8029e-01, 4.7403e-01, 4.2889e-01,\n        3.5084e-01, 1.4187e-01, 4.4056e-01, 5.7033e-01, 4.9778e-01, 8.1038e-01,\n        5.7606e-01, 4.8116e-01, 2.2426e-01, 8.7398e-01, 6.2634e-01, 3.8769e-01,\n        8.7170e-01, 6.6032e-01, 4.8786e-03, 6.0990e-02, 1.2778e-01, 2.9638e-01,\n        2.0806e-01, 1.3343e-01, 2.5414e-02, 4.1106e-01, 8.5384e-01, 8.7712e-01,\n        1.7310e-02, 6.8996e-02, 3.2370e-02, 9.8035e-01, 9.3215e-01, 5.9434e-01,\n        8.0573e-01, 3.1583e-01, 3.4815e-01, 8.9839e-01, 2.7450e-02, 9.3439e-01,\n        7.4615e-01, 1.6936e-01, 3.9897e-01, 9.2557e-01, 6.1975e-01, 6.9744e-01,\n        3.9812e-01, 7.6628e-01, 4.6205e-01, 2.5668e-01, 3.5637e-01, 9.7589e-01,\n        9.7539e-01, 6.9650e-02, 4.2055e-01, 5.2815e-01, 4.1325e-01, 6.0469e-01,\n        9.5184e-01, 5.7837e-01, 6.8111e-01, 3.9773e-01, 4.7398e-01, 9.6995e-01,\n        8.9349e-02, 7.1366e-01, 3.8643e-01, 9.4546e-01, 3.3062e-01, 7.4520e-03,\n        7.8092e-03, 7.6223e-01, 8.8274e-01, 6.8050e-01, 4.3685e-01, 1.0996e-01,\n        4.7560e-01, 9.3365e-02, 1.7165e-01, 1.2536e-01, 9.9105e-01, 9.9123e-01,\n        5.2001e-01, 1.8239e-01, 7.0742e-01, 6.7226e-01, 4.2759e-01, 5.6319e-01,\n        4.2051e-01, 6.9120e-02, 6.6342e-01, 9.5281e-01, 4.3755e-01, 9.5999e-01,\n        7.5591e-01, 8.3064e-01, 8.3476e-01, 7.4947e-01, 5.9000e-01, 2.1103e-01,\n        5.0039e-01, 2.7622e-01, 6.3914e-01, 4.6453e-01, 4.8232e-04, 4.8678e-01,\n        7.0276e-01, 1.2048e-01, 3.0000e-01, 3.7730e-01, 6.1310e-03, 5.1246e-01,\n        9.7980e-01, 4.6687e-01, 2.7049e-01, 4.2934e-01, 7.8575e-01, 3.0175e-01,\n        1.4535e-01, 1.4961e-01, 4.6824e-01, 1.4982e-01, 3.5293e-01, 5.0061e-01,\n        4.2234e-02, 5.3242e-01, 3.9917e-01, 7.9273e-01, 5.7077e-01, 9.3541e-01,\n        9.7752e-03, 2.6741e-01, 5.3530e-01, 9.5146e-01, 7.4037e-01, 7.2323e-01,\n        9.4544e-01, 8.1587e-01, 9.6551e-01, 5.8702e-01, 4.4200e-01, 9.1728e-01,\n        8.0925e-01, 7.1473e-01, 6.1602e-01, 6.0771e-01, 5.2970e-01, 1.9444e-01,\n        7.7686e-01, 6.8607e-01, 8.7017e-01, 9.3810e-01, 2.0961e-01, 8.0898e-02,\n        8.2443e-01, 5.1144e-01, 4.7545e-01, 2.8544e-01, 3.9306e-01, 6.0405e-01,\n        3.9088e-01, 7.0508e-01, 6.8278e-01, 2.4696e-01, 5.5319e-01, 5.2963e-01,\n        6.5997e-01, 3.6849e-01, 2.3453e-02, 3.3473e-01, 8.8468e-01, 7.7119e-01,\n        5.2400e-01, 1.6325e-01, 8.8217e-01, 8.3910e-01, 6.4840e-01, 4.8592e-01,\n        4.2074e-01, 7.1069e-01, 9.9769e-02, 1.2881e-01, 7.6820e-01, 3.0431e-01,\n        6.0925e-01, 5.2847e-01, 3.3189e-01, 9.5496e-01, 9.5380e-01, 5.0207e-01,\n        4.0477e-01, 4.7102e-01, 6.4067e-01, 1.5120e-01, 4.2124e-01, 9.2751e-01,\n        3.4550e-01, 6.8743e-02, 4.4667e-01, 1.9811e-01, 1.8481e-02, 9.1619e-01,\n        5.7966e-03, 8.0385e-01, 7.0425e-01, 3.7842e-01, 9.6244e-01, 3.7289e-02,\n        5.1059e-01, 6.5691e-02, 6.1693e-01, 5.0997e-01, 9.0126e-01, 7.0149e-02,\n        4.5577e-01, 6.7399e-01, 7.8183e-01, 5.1805e-01, 8.2415e-01, 7.2963e-01,\n        9.3829e-01, 6.2880e-01, 3.0464e-01, 2.9609e-01, 7.1180e-04, 8.5032e-01,\n        8.8195e-01, 6.2198e-01, 6.6600e-02, 3.5440e-01, 3.9870e-01, 7.1794e-01,\n        2.5072e-01, 5.3895e-02, 3.7280e-01, 1.2637e-02, 4.2150e-01, 4.4096e-02,\n        4.0387e-01, 4.6432e-01, 6.2618e-02, 1.8862e-01, 4.7754e-01, 4.4415e-01,\n        2.5383e-01, 3.7587e-01, 9.1606e-01, 7.8984e-01, 2.6505e-01, 2.6363e-01,\n        2.5995e-01, 5.7268e-01, 3.8814e-01, 7.7290e-01, 9.0941e-01, 4.2257e-01,\n        1.5339e-01, 4.7617e-01, 1.4643e-01, 9.4751e-01, 8.5553e-01, 5.2902e-01,\n        6.3557e-01, 1.0439e-01, 2.8087e-01, 5.7533e-01, 4.0643e-01, 9.6533e-02,\n        2.0074e-01, 6.1255e-01, 6.1688e-01, 6.7355e-01, 2.7356e-01, 3.0159e-01,\n        6.9753e-01, 4.0380e-01, 1.4077e-01, 5.2356e-01, 6.5850e-01, 8.6789e-01,\n        1.9443e-01, 9.3979e-01, 7.8556e-01, 4.0803e-01, 4.9676e-01, 1.2908e-01,\n        4.9485e-01, 1.7348e-01, 6.6231e-01, 1.7308e-01, 6.4689e-01, 9.7923e-01,\n        1.0211e-01, 1.6433e-01, 3.6372e-01, 1.2851e-01, 4.7264e-01, 9.6455e-01,\n        2.2765e-01, 3.8615e-01, 1.0712e-01, 8.6763e-02, 4.8813e-01, 8.9692e-01,\n        5.7527e-01, 2.6399e-01, 2.5632e-01, 3.7566e-01, 5.2084e-01, 4.7470e-01,\n        9.7930e-01, 3.5166e-01, 9.1898e-01, 8.4318e-01, 4.9218e-01, 8.6635e-01,\n        9.6121e-01, 9.9477e-01, 7.0478e-01, 7.4428e-01, 2.0494e-01, 1.6206e-01,\n        2.2478e-01, 3.0600e-01, 4.9878e-01, 2.2007e-01, 2.4453e-02, 6.6675e-01,\n        9.0109e-01, 7.9572e-01, 3.4442e-01, 9.6500e-01, 9.4492e-01, 8.4520e-01,\n        1.4979e-01, 4.5379e-01, 2.1712e-01, 9.0614e-02, 5.5350e-01, 2.8294e-01,\n        6.7821e-02, 1.8451e-01, 6.1734e-01, 5.0501e-01, 6.0500e-01, 4.8678e-01,\n        7.7853e-01, 1.0100e-01, 9.3932e-01, 9.6433e-01, 3.2737e-01, 5.0535e-01])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([432])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.ravel().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "432"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h * w * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Посчитаем количество элементов в тензоре с помощью [специальной функции](https://pytorch.org/docs/stable/generated/torch.numel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "432"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.8459, 0.5923, 0.4834],\n         [0.2859, 0.0078, 0.5961]],\n\n        [[0.5555, 0.7200, 0.3974],\n         [0.1111, 0.6455, 0.0701]],\n\n        [[0.0949, 0.9467, 0.0756],\n         [0.3132, 0.6796, 0.4059]]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 2\n",
    "w = 3\n",
    "c = 3\n",
    "\n",
    "shape = (c, h, w)\n",
    "\n",
    "image_tensor = torch.rand(shape)\n",
    "\n",
    "image_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Попробуем поменять размер с помощью функции [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.8459, 0.5923, 0.4834, 0.2859, 0.0078, 0.5961],\n        [0.5555, 0.7200, 0.3974, 0.1111, 0.6455, 0.0701],\n        [0.0949, 0.9467, 0.0756, 0.3132, 0.6796, 0.4059]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.reshape(c, h * w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Попробуем собрать из нескольких тензоров один большой:\n",
    "\n",
    "[torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0626, -0.3343,  0.0460],\n        [-1.5721,  1.0659,  0.3535]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0626, -0.3343,  0.0460],\n        [-1.5721,  1.0659,  0.3535],\n        [-1.0626, -0.3343,  0.0460],\n        [-1.5721,  1.0659,  0.3535],\n        [-1.0626, -0.3343,  0.0460],\n        [-1.5721,  1.0659,  0.3535]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0626, -0.3343,  0.0460, -1.0626, -0.3343,  0.0460, -1.0626, -0.3343,\n          0.0460],\n        [-1.5721,  1.0659,  0.3535, -1.5721,  1.0659,  0.3535, -1.5721,  1.0659,\n          0.3535]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, x, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5081, -0.0433, -1.0095],\n",
      "        [ 0.8072,  0.0686, -0.5956],\n",
      "        [-0.1236, -1.5279, -0.1828]])\n",
      "tensor([[ 0.2385, -1.4352, -0.0794],\n",
      "        [-1.7952,  0.4250,  0.3012],\n",
      "        [ 0.9589,  0.1381,  0.8807],\n",
      "        [-1.2794,  1.5235, -0.1737],\n",
      "        [ 0.1405,  0.8786,  0.9615]])\n",
      "tensor([[ 0.2161,  0.6020, -2.7303]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-0.5081, -0.0433, -1.0095],\n        [ 0.8072,  0.0686, -0.5956],\n        [-0.1236, -1.5279, -0.1828],\n        [ 0.2385, -1.4352, -0.0794],\n        [-1.7952,  0.4250,  0.3012],\n        [ 0.9589,  0.1381,  0.8807],\n        [-1.2794,  1.5235, -0.1737],\n        [ 0.1405,  0.8786,  0.9615],\n        [ 0.2161,  0.6020, -2.7303]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(5, 3)\n",
    "z = torch.randn(1, 3)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4959,  1.1638, -0.4940],\n",
      "        [-1.1387, -0.8416,  0.7781]])\n",
      "tensor([[-1.1391, -0.3126,  1.9179, -0.2388,  0.5275],\n",
      "        [-0.3563, -0.3405,  0.7361,  0.4530,  1.3467]])\n",
      "tensor([[0.2098],\n",
      "        [1.5773]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-1.4959,  1.1638, -0.4940, -1.1391, -0.3126,  1.9179, -0.2388,  0.5275,\n          0.2098],\n        [-1.1387, -0.8416,  0.7781, -0.3563, -0.3405,  0.7361,  0.4530,  1.3467,\n          1.5773]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 5)\n",
    "z = torch.randn(2, 1)\n",
    "\n",
    "for tensor in [x, y, z]:\n",
    "    print(tensor)\n",
    "\n",
    "torch.cat((x, y, z), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь добавим дополнительную ось:\n",
    "\n",
    "[torch.unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7767, 0.9918, 0.3863],\n",
      "        [0.5592, 0.2939, 0.3967]])\n",
      "\n",
      "tensor([[[0.7767, 0.9918, 0.3863],\n",
      "         [0.5592, 0.2939, 0.3967]]]) torch.Size([1, 2, 3])\n",
      "\n",
      "tensor([[[0.7767, 0.9918, 0.3863]],\n",
      "\n",
      "        [[0.5592, 0.2939, 0.3967]]]) torch.Size([2, 1, 3])\n",
      "\n",
      "tensor([[[0.7767],\n",
      "         [0.9918],\n",
      "         [0.3863]],\n",
      "\n",
      "        [[0.5592],\n",
      "         [0.2939],\n",
      "         [0.3967]]]) torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.unsqueeze(0), x.unsqueeze(0).shape)\n",
    "print()\n",
    "print(x.unsqueeze(1), x.unsqueeze(1).shape)\n",
    "print()\n",
    "print(x.unsqueeze(2), x.unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Уберем лишние оси (где размер единичка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.7464, 0.2264, 0.8477]],\n",
      "\n",
      "         [[0.8924, 0.3718, 0.4036]]]])\n",
      "\n",
      "tensor([[0.7464, 0.2264, 0.8477],\n",
      "        [0.8924, 0.3718, 0.4036]]) torch.Size([2, 3])\n",
      "\n",
      "tensor([[[0.7464, 0.2264, 0.8477]],\n",
      "\n",
      "        [[0.8924, 0.3718, 0.4036]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 2, 1, 3)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(x.squeeze(), x.squeeze().shape)\n",
    "print()\n",
    "print(x.squeeze(0), x.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь поговорим про типы данных в тензорах. По умолчанию в тензорах лежат числа в torch.float32 для вещественных и torch.int64 для целочисленных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.1992, 3.6992, 4.8984], dtype=torch.float16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 2.2000, 3.7000, 4.9000], dtype=torch.float64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([1.5, 2.2, 3.7, 4.9], dtype=torch.float64)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int32)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], dtype=torch.int16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], dtype=torch.int16)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Размещение тензора на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [33], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available())\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_device_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\cuda\\__init__.py:341\u001B[0m, in \u001B[0;36mget_device_name\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_device_name\u001B[39m(device: Optional[_device_t] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Gets the name of a device.\u001B[39;00m\n\u001B[0;32m    331\u001B[0m \n\u001B[0;32m    332\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m        str: the name of the device\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 341\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_device_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mname\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\cuda\\__init__.py:371\u001B[0m, in \u001B[0;36mget_device_properties\u001B[1;34m(device)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_device_properties\u001B[39m(device: _device_t) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _CudaDeviceProperties:\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Gets the properties of a device.\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \n\u001B[0;32m    364\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001B[39;00m\n\u001B[0;32m    370\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 371\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# will define _get_device_properties\u001B[39;00m\n\u001B[0;32m    372\u001B[0m     device \u001B[38;5;241m=\u001B[39m _get_device_index(device, optional\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m device \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m device_count():\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\cuda\\__init__.py:221\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m--> 221\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    223\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 11 21:46:39 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:09:00.0  On |                    0 |\r\n",
      "| 30%   32C    P5    31W / 450W |    715MiB / 23028MiB |     37%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1952      G   /usr/lib/xorg/Xorg                509MiB |\r\n",
      "|    0   N/A  N/A      2177      G   /usr/bin/gnome-shell              156MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([15, 22, 37, 49])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49], device=device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 22, 37, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([15, 22, 37, 49])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([15, 22, 37, 49])\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "tensor = tensor.to(device)\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 22, 37, 49])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor.cpu()\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.1656, 1.5659, 0.1778],\n        [1.5164, 0.2269, 1.4854]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.1656, 1.5659, 0.1778],\n        [1.5164, 0.2269, 1.4854]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.1656, 1.5659, 0.1778],\n        [1.5164, 0.2269, 1.4854]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.to(device)\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Операции с тензорами\n",
    "\n",
    "Большая часть операций с тензорами хорошо описана в их [документации](https://pytorch.org/docs/stable/torch.html), разберем основные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0542, 0.3823, 0.5019],\n",
       "         [0.7535, 0.0122, 0.9364]]),\n",
       " tensor([[0.9840, 0.0882, 0.7651],\n",
       "         [0.7870, 0.4985, 0.0727]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1656, 1.5659, 0.1778],\n",
      "        [1.5164, 0.2269, 1.4854]])\n",
      "\n",
      "tensor([[1.1656, 1.5659, 0.1778],\n",
      "        [1.5164, 0.2269, 1.4854]])\n",
      "\n",
      "tensor([[1.1656, 1.5659, 0.1778],\n",
      "        [1.5164, 0.2269, 1.4854]])\n"
     ]
    }
   ],
   "source": [
    "# поэлементные\n",
    "\n",
    "print(a + b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.add(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.add(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n",
      "\n",
      "tensor([[-0.9298,  0.2940, -0.2632],\n",
      "        [-0.0335, -0.4864,  0.8637]])\n"
     ]
    }
   ],
   "source": [
    "print(a - b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.sub(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.sub(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n",
      "\n",
      "tensor([[0.0534, 0.0337, 0.3840],\n",
      "        [0.5929, 0.0061, 0.0680]])\n"
     ]
    }
   ],
   "source": [
    "print(a * b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.mul(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.mul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n",
      "\n",
      "tensor([[ 0.0551,  4.3328,  0.6560],\n",
      "        [ 0.9574,  0.0244, 12.8874]])\n"
     ]
    }
   ],
   "source": [
    "print(a / b)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.div(a, b))\n",
    "\n",
    "print()\n",
    "\n",
    "print(a.div(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.7839, 0.7445, 0.1663],\n         [0.1771, 0.2565, 0.9236]]),\n tensor([[0.5498, 0.2048, 0.3006, 0.1585],\n         [0.9301, 0.3197, 0.2234, 0.2159],\n         [0.6899, 0.1117, 0.4555, 0.2688]]),\n tensor([[0.5205, 0.1241, 0.4093, 0.0631, 0.3467],\n         [0.3682, 0.3803, 0.2722, 0.6547, 0.3566],\n         [0.1898, 0.5560, 0.3110, 0.2315, 0.0014],\n         [0.3413, 0.1767, 0.6008, 0.2367, 0.3634],\n         [0.0293, 0.3353, 0.1653, 0.5538, 0.9967]]))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 4)\n",
    "c = torch.rand(5, 5)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2382, 0.4171, 0.4777, 0.3297],\n",
      "        [0.9731, 0.2214, 0.5312, 0.3317]]) torch.Size([2, 4])\n",
      "\n",
      "tensor([[1.2382, 0.4171, 0.4777, 0.3297],\n",
      "        [0.9731, 0.2214, 0.5312, 0.3317]]) torch.Size([2, 4])\n",
      "\n",
      "tensor(2.4452)\n",
      "\n",
      "tensor([[1.6829, 1.1321, 1.5058, 1.0651, 1.4144],\n",
      "        [1.4451, 1.4627, 1.3128, 1.9245, 1.4284],\n",
      "        [1.2089, 1.7437, 1.3647, 1.2604, 1.0014],\n",
      "        [1.4068, 1.1933, 1.8235, 1.2671, 1.4382],\n",
      "        [1.0298, 1.3983, 1.1798, 1.7398, 2.7094]])\n"
     ]
    }
   ],
   "source": [
    "# матричные операции\n",
    "\n",
    "print(a @ b, (a @ b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(torch.matmul(a, b), torch.matmul(a, b).shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.trace())\n",
    "\n",
    "print()\n",
    "\n",
    "print(c.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### [Автоматическое дифференцирование](https://pytorch.org/docs/stable/notes/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.8231, 0.7329, 0.0158, 0.2736, 0.0394])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4069, 0.6291, 0.6486, 0.5699, 0.0183],\n        [0.8054, 0.7856, 0.8484, 0.2544, 0.1522],\n        [0.0918, 0.2401, 0.9071, 0.9485, 0.9259]], requires_grad=True)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.4013e-45,  0.0000e+00, -5.4306e-33])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_z = torch.empty(3)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9629, 1.3277, 0.5618], grad_fn=<CopySlices>)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    first_z[i] = torch.sum(w[i] * x)\n",
    "\n",
    "first_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9629, 1.3277, 0.5618], grad_fn=<SqueezeBackward3>)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.matmul(x, w.t())\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.6110, 0.7522, 0.1431], requires_grad=True)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.rand(3, requires_grad=True)\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.6673, grad_fn=<SumBackward0>)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.sum(z * v)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1.6673293113708496"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss = torch.mean((y - 2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.1107, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=None\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuke/.local/lib/python3.10/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad=None\n",
      "\n",
      "w.grad=tensor([[-0.3346, -0.2979, -0.0064, -0.1112, -0.0160],\n",
      "        [-0.4119, -0.3668, -0.0079, -0.1369, -0.0197],\n",
      "        [-0.0783, -0.0698, -0.0015, -0.0260, -0.0037]])\n",
      "\n",
      "z.grad=None\n",
      "\n",
      "v.grad=tensor([-0.6407, -0.8833, -0.3738])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Диман\\AppData\\Local\\Temp\\ipykernel_9300\\443451818.py:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:485.)\n",
      "  print(f'{z.grad=}\\n')\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.grad=}\\n')\n",
    "print(f'{w.grad=}\\n')\n",
    "print(f'{z.grad=}\\n')\n",
    "print(f'{v.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0.0586], requires_grad=True), tensor([0.2504], requires_grad=True))"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.1918], grad_fn=<SubBackward0>)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([1.])\n",
      "\n",
      "b.grad=tensor([-1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 1\n",
    "print(f'{b.grad=}\\n')  # -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad.zero_()\n",
    "b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0368], grad_fn=<PowBackward0>)"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (a - b) ** 2\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([0.])\n",
      "\n",
      "b.grad=tensor([0.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([-0.3835])\n",
      "\n",
      "b.grad=tensor([0.3835])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # 2 * (a - b)\n",
    "print(f'{b.grad=}\\n')  # -2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.3835], grad_fn=<MulBackward0>)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3279, 0.5942, 0.6495, 0.8076, 0.6961],\n",
       "         [0.7243, 0.3865, 0.4297, 0.0069, 0.3211],\n",
       "         [0.9084, 0.0009, 0.5393, 0.4543, 0.1057]], requires_grad=True),\n",
       " tensor([[0.3158, 0.7038, 0.3901, 0.7456, 0.8604],\n",
       "         [0.4813, 0.8476, 0.9554, 0.9591, 0.9958],\n",
       "         [0.5856, 0.4295, 0.9476, 0.6895, 0.3585]], requires_grad=True))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3189, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.mean(a * b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
      "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
      "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]])\n",
      "\n",
      "b.grad=tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
      "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
      "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # b / (3 * 5)\n",
    "print(f'{b.grad=}\\n')  # a / (3 * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1862e-02, 3.9616e-02, 4.3297e-02, 5.3840e-02, 4.6404e-02],\n",
       "        [4.8289e-02, 2.5765e-02, 2.8644e-02, 4.6089e-04, 2.1404e-02],\n",
       "        [6.0560e-02, 6.2525e-05, 3.5952e-02, 3.0284e-02, 7.0479e-03]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0211, 0.0469, 0.0260, 0.0497, 0.0574],\n",
       "        [0.0321, 0.0565, 0.0637, 0.0639, 0.0664],\n",
       "        [0.0390, 0.0286, 0.0632, 0.0460, 0.0239]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0.3089, 0.1212, 0.7699, 0.2991, 0.6856],\n",
      "        [0.8254, 0.8313, 0.5763, 0.8633, 0.4978],\n",
      "        [0.9863, 0.7055, 0.7012, 0.4828, 0.2239]], requires_grad=True)\n",
      "\n",
      "a.grad=None\n",
      "\n",
      "a.grad=tensor([[0.6178, 0.2425, 1.5398, 0.5982, 1.3711],\n",
      "        [1.6509, 1.6625, 1.1526, 1.7267, 0.9956],\n",
      "        [1.9726, 1.4110, 1.4024, 0.9656, 0.4478]])\n",
      "\n",
      "a.grad=tensor([[1.6178, 1.2425, 2.5398, 1.5982, 2.3711],\n",
      "        [2.6509, 2.6625, 2.1526, 2.7267, 1.9956],\n",
      "        [2.9726, 2.4110, 2.4024, 1.9656, 1.4478]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "print(f'{a=}\\n')\n",
    "\n",
    "loss1 = torch.sum(a ** 2) # 2a\n",
    "loss2 = torch.sum(a) # 1\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss1.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')\n",
    "\n",
    "loss2.backward()\n",
    "\n",
    "print(f'{a.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*a=tensor([[0.6178, 0.2425, 1.5398, 0.5982, 1.3711],\n",
      "        [1.6509, 1.6625, 1.1526, 1.7267, 0.9956],\n",
      "        [1.9726, 1.4110, 1.4024, 0.9656, 0.4478]], grad_fn=<MulBackward0>)\n",
      "\n",
      "2*a+1=tensor([[1.6178, 1.2425, 2.5398, 1.5982, 2.3711],\n",
      "        [2.6509, 2.6625, 2.1526, 2.7267, 1.9956],\n",
      "        [2.9726, 2.4110, 2.4024, 1.9656, 1.4478]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'{2*a=}\\n')\n",
    "print(f'{2*a+1=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.7667, 0.0997, 0.7116, 0.3963, 0.5159],\n         [0.2856, 0.2517, 0.7958, 0.9150, 0.1474],\n         [0.5395, 0.5908, 0.0233, 0.7836, 0.2381]], requires_grad=True),\n tensor([[0.5585, 0.3541, 0.7897, 0.3240, 0.8873],\n         [0.5724, 0.6629, 0.2364, 0.9608, 0.6402],\n         [0.9968, 0.1512, 0.8575, 0.2994, 0.1174]]))"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=False)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-1.3475, grad_fn=<SumBackward0>)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')  # all ones\n",
    "print(f'{b.grad=}\\n')  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.4642, 0.8980, 0.2700, 0.0618, 0.2028],\n         [0.3148, 0.3265, 0.9826, 0.5633, 0.7017],\n         [0.0217, 0.3777, 0.7322, 0.4817, 0.0629]], requires_grad=True),\n tensor([[0.3316, 0.1806, 0.0930, 0.1765, 0.6865],\n         [0.4488, 0.3475, 0.5174, 0.9252, 0.6275],\n         [0.5770, 0.2560, 0.5576, 0.7211, 0.6940]], requires_grad=True))"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.6784)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [81], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.0455, 0.8142, 0.4499, 0.1309, 0.0169],\n         [0.2717, 0.4404, 0.2068, 0.5862, 0.9953],\n         [0.3041, 0.4199, 0.8984, 0.2433, 0.3278]], requires_grad=True),\n tensor([[0.4343, 0.4018, 0.8798, 0.6746, 0.6699],\n         [0.8597, 0.8055, 0.7975, 0.8371, 0.2015],\n         [0.4817, 0.6337, 0.1011, 0.9475, 0.3228]], requires_grad=True))"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3, 5, requires_grad=True)\n",
    "b = torch.rand(3, 5, requires_grad=True)\n",
    "\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-2.8974)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    loss = torch.sum(a - b)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [84], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(15.0416)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [85], line 9\u001B[0m\n\u001B[0;32m      5\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(a \u001B[38;5;241m+\u001B[39m b)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(15.0416, grad_fn=<SumBackward0>)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=None\n",
      "\n",
      "b.grad=None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "b.grad=tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{a.grad=}\\n')\n",
    "print(f'{b.grad=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(12.4514)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [90], line 9\u001B[0m\n\u001B[0;32m      5\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(a \u001B[38;5;241m+\u001B[39m b)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.sum(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(12.4514)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = torch.sum(a + b)\n",
    "\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(0.9060)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0734, grad_fn=<MeanBackward0>)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def foo():\n",
    "    a = torch.rand(3, 5, requires_grad=True)\n",
    "    b = torch.rand(3, 5, requires_grad=True)\n",
    "    \n",
    "    loss = torch.mean(a + b)\n",
    "    \n",
    "    print(f'{loss=}')\n",
    "    \n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(1.2167)\n"
     ]
    }
   ],
   "source": [
    "a, b = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0245)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Полносвязные слои и функции активации в `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Полносвязный слой\n",
    "\n",
    ">$y_j = \\sum\\limits_{i=1}^{n}x_iw_{ji} + b_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=5, out_features=3, bias=True)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.4345, -0.0645,  0.0373,  0.1707, -0.2128],\n        [-0.2186, -0.1971, -0.1130,  0.4246, -0.3682],\n        [ 0.0943,  0.2426, -0.2593, -0.1868,  0.3203]], requires_grad=True)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 5])"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([-0.2694,  0.3204,  0.0024], requires_grad=True)"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer = nn.Linear(in_features=5, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module._call_impl of Linear(in_features=5, out_features=3, bias=False)>"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5176,  0.3132, -1.2666], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Сигмоида $f(x) = \\dfrac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.9201,  1.5108, -1.7982,  0.6531,  0.3406])\n",
      "tensor([0.1279, 0.8192, 0.1421, 0.6577, 0.5843])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> ReLU $f(x) = \\max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3181, -0.6560, -2.6934,  0.7624, -0.6946])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.7624, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "> Leaky ReLU $f(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "activation = nn.LeakyReLU(negative_slope=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3040, -0.8739,  1.0615, -0.6037, -0.1920])\n",
      "tensor([ 3.0397e-01, -8.7389e-04,  1.0615e+00, -6.0367e-04, -1.9199e-04])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(activation(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Градиентный спуск своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "b_true = torch.randn(1)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects) + b_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 200\n",
    "step_size = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 130.75197\n",
      "MSE на шаге 2 53.79208\n",
      "MSE на шаге 3 27.54351\n",
      "MSE на шаге 4 17.58543\n",
      "MSE на шаге 5 13.15989\n",
      "MSE на шаге 6 10.80943\n",
      "MSE на шаге 7 9.35913\n",
      "MSE на шаге 8 8.36826\n",
      "MSE на шаге 9 7.64593\n",
      "MSE на шаге 10 7.09519\n",
      "MSE на шаге 11 6.65989\n",
      "MSE на шаге 12 6.30461\n",
      "MSE на шаге 13 6.00597\n",
      "MSE на шаге 14 5.74817\n",
      "MSE на шаге 15 5.52038\n",
      "MSE на шаге 16 5.31513\n",
      "MSE на шаге 17 5.12724\n",
      "MSE на шаге 18 4.95309\n",
      "MSE на шаге 19 4.79012\n",
      "MSE на шаге 20 4.63651\n",
      "MSE на шаге 21 4.49096\n",
      "MSE на шаге 31 3.34447\n",
      "MSE на шаге 41 2.58160\n",
      "MSE на шаге 51 2.06947\n",
      "MSE на шаге 61 1.72556\n",
      "MSE на шаге 71 1.49461\n",
      "MSE на шаге 81 1.33952\n",
      "MSE на шаге 91 1.23537\n",
      "MSE на шаге 101 1.16543\n",
      "MSE на шаге 111 1.11847\n",
      "MSE на шаге 121 1.08693\n",
      "MSE на шаге 131 1.06575\n",
      "MSE на шаге 141 1.05153\n",
      "MSE на шаге 151 1.04197\n",
      "MSE на шаге 161 1.03556\n",
      "MSE на шаге 171 1.03125\n",
      "MSE на шаге 181 1.02836\n",
      "MSE на шаге 191 1.02642\n"
     ]
    }
   ],
   "source": [
    "w = torch.rand(n_features, requires_grad=True)\n",
    "b = torch.rand(1, requires_grad=True)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * step_size\n",
    "        b -= b.grad * step_size\n",
    "\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 67.76115\n",
      "MSE на шаге 2 42.90440\n",
      "MSE на шаге 3 34.78294\n",
      "MSE на шаге 4 31.90576\n",
      "MSE на шаге 5 30.72448\n",
      "MSE на шаге 6 30.12742\n",
      "MSE на шаге 7 29.75548\n",
      "MSE на шаге 8 29.48539\n",
      "MSE на шаге 9 29.27014\n",
      "MSE на шаге 10 29.08890\n",
      "MSE на шаге 11 28.93083\n",
      "MSE на шаге 12 28.78951\n",
      "MSE на шаге 13 28.66082\n",
      "MSE на шаге 14 28.54199\n",
      "MSE на шаге 15 28.43109\n",
      "MSE на шаге 16 28.32677\n",
      "MSE на шаге 17 28.22807\n",
      "MSE на шаге 18 28.13428\n",
      "MSE на шаге 19 28.04487\n",
      "MSE на шаге 20 27.95945\n",
      "MSE на шаге 21 27.87770\n",
      "MSE на шаге 31 27.22173\n",
      "MSE на шаге 41 26.78239\n",
      "MSE на шаге 51 26.48737\n",
      "MSE на шаге 61 26.28927\n",
      "MSE на шаге 71 26.15623\n",
      "MSE на шаге 81 26.06689\n",
      "MSE на шаге 91 26.00690\n",
      "MSE на шаге 101 25.96661\n",
      "MSE на шаге 111 25.93955\n",
      "MSE на шаге 121 25.92138\n",
      "MSE на шаге 131 25.90918\n",
      "MSE на шаге 141 25.90099\n",
      "MSE на шаге 151 25.89549\n",
      "MSE на шаге 161 25.89179\n",
      "MSE на шаге 171 25.88931\n",
      "MSE на шаге 181 25.88764\n",
      "MSE на шаге 191 25.88652\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x)\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "#     layer.weight.grad.zero_()\n",
    "#     layer.bias.grad.zero_()\n",
    "    \n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7298],\n",
       "        [-2.7292],\n",
       "        [-2.7318],\n",
       "        [-2.7311],\n",
       "        [-2.7278],\n",
       "        [-2.7308],\n",
       "        [-2.7292],\n",
       "        [-2.7330],\n",
       "        [-2.7296],\n",
       "        [-2.7168],\n",
       "        [-2.7313],\n",
       "        [-2.7349],\n",
       "        [-2.7305],\n",
       "        [-2.7286],\n",
       "        [-2.7363],\n",
       "        [-2.7269],\n",
       "        [-2.7251],\n",
       "        [-2.7222],\n",
       "        [-2.7308],\n",
       "        [-2.7260],\n",
       "        [-2.7310],\n",
       "        [-2.7279],\n",
       "        [-2.7256],\n",
       "        [-2.7202],\n",
       "        [-2.7229],\n",
       "        [-2.7306],\n",
       "        [-2.7265],\n",
       "        [-2.7337],\n",
       "        [-2.7233],\n",
       "        [-2.7287],\n",
       "        [-2.7279],\n",
       "        [-2.7318],\n",
       "        [-2.7289],\n",
       "        [-2.7257],\n",
       "        [-2.7223],\n",
       "        [-2.7243],\n",
       "        [-2.7390],\n",
       "        [-2.7176],\n",
       "        [-2.7257],\n",
       "        [-2.7269],\n",
       "        [-2.7223],\n",
       "        [-2.7276],\n",
       "        [-2.7244],\n",
       "        [-2.7321],\n",
       "        [-2.7252],\n",
       "        [-2.7309],\n",
       "        [-2.7242],\n",
       "        [-2.7299],\n",
       "        [-2.7348],\n",
       "        [-2.7298],\n",
       "        [-2.7290],\n",
       "        [-2.7337],\n",
       "        [-2.7367],\n",
       "        [-2.7189],\n",
       "        [-2.7300],\n",
       "        [-2.7280],\n",
       "        [-2.7253],\n",
       "        [-2.7337],\n",
       "        [-2.7322],\n",
       "        [-2.7274],\n",
       "        [-2.7266],\n",
       "        [-2.7305],\n",
       "        [-2.7325],\n",
       "        [-2.7221],\n",
       "        [-2.7273],\n",
       "        [-2.7294],\n",
       "        [-2.7239],\n",
       "        [-2.7206],\n",
       "        [-2.7323],\n",
       "        [-2.7229],\n",
       "        [-2.7279],\n",
       "        [-2.7308],\n",
       "        [-2.7320],\n",
       "        [-2.7193],\n",
       "        [-2.7196],\n",
       "        [-2.7275],\n",
       "        [-2.7284],\n",
       "        [-2.7304],\n",
       "        [-2.7291],\n",
       "        [-2.7330],\n",
       "        [-2.7342],\n",
       "        [-2.7195],\n",
       "        [-2.7230],\n",
       "        [-2.7376],\n",
       "        [-2.7266],\n",
       "        [-2.7182],\n",
       "        [-2.7261],\n",
       "        [-2.7240],\n",
       "        [-2.7260],\n",
       "        [-2.7287],\n",
       "        [-2.7365],\n",
       "        [-2.7253],\n",
       "        [-2.7333],\n",
       "        [-2.7305],\n",
       "        [-2.7338],\n",
       "        [-2.7322],\n",
       "        [-2.7306],\n",
       "        [-2.7302],\n",
       "        [-2.7355],\n",
       "        [-2.7345],\n",
       "        [-2.7258],\n",
       "        [-2.7270],\n",
       "        [-2.7259],\n",
       "        [-2.7286],\n",
       "        [-2.7312],\n",
       "        [-2.7181],\n",
       "        [-2.7331],\n",
       "        [-2.7254],\n",
       "        [-2.7366],\n",
       "        [-2.7318],\n",
       "        [-2.7297],\n",
       "        [-2.7317],\n",
       "        [-2.7312],\n",
       "        [-2.7204],\n",
       "        [-2.7212],\n",
       "        [-2.7324],\n",
       "        [-2.7290],\n",
       "        [-2.7303],\n",
       "        [-2.7195],\n",
       "        [-2.7187],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7204],\n",
       "        [-2.7305],\n",
       "        [-2.7373],\n",
       "        [-2.7283],\n",
       "        [-2.7349],\n",
       "        [-2.7276],\n",
       "        [-2.7236],\n",
       "        [-2.7314],\n",
       "        [-2.7231],\n",
       "        [-2.7292],\n",
       "        [-2.7359],\n",
       "        [-2.7278],\n",
       "        [-2.7184],\n",
       "        [-2.7301],\n",
       "        [-2.7245],\n",
       "        [-2.7376],\n",
       "        [-2.7306],\n",
       "        [-2.7318],\n",
       "        [-2.7294],\n",
       "        [-2.7340],\n",
       "        [-2.7255],\n",
       "        [-2.7266],\n",
       "        [-2.7193],\n",
       "        [-2.7233],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7229],\n",
       "        [-2.7287],\n",
       "        [-2.7244],\n",
       "        [-2.7332],\n",
       "        [-2.7236],\n",
       "        [-2.7283],\n",
       "        [-2.7318],\n",
       "        [-2.7252],\n",
       "        [-2.7279],\n",
       "        [-2.7341],\n",
       "        [-2.7271],\n",
       "        [-2.7242],\n",
       "        [-2.7222],\n",
       "        [-2.7299],\n",
       "        [-2.7236],\n",
       "        [-2.7349],\n",
       "        [-2.7271],\n",
       "        [-2.7251],\n",
       "        [-2.7279],\n",
       "        [-2.7245],\n",
       "        [-2.7366],\n",
       "        [-2.7269],\n",
       "        [-2.7339],\n",
       "        [-2.7342],\n",
       "        [-2.7311],\n",
       "        [-2.7286],\n",
       "        [-2.7327],\n",
       "        [-2.7341],\n",
       "        [-2.7282],\n",
       "        [-2.7376],\n",
       "        [-2.7284],\n",
       "        [-2.7178],\n",
       "        [-2.7322],\n",
       "        [-2.7360],\n",
       "        [-2.7312],\n",
       "        [-2.7323],\n",
       "        [-2.7257],\n",
       "        [-2.7263],\n",
       "        [-2.7201],\n",
       "        [-2.7289],\n",
       "        [-2.7302],\n",
       "        [-2.7270],\n",
       "        [-2.7328],\n",
       "        [-2.7259],\n",
       "        [-2.7294],\n",
       "        [-2.7368],\n",
       "        [-2.7221],\n",
       "        [-2.7358],\n",
       "        [-2.7235],\n",
       "        [-2.7357],\n",
       "        [-2.7213],\n",
       "        [-2.7244],\n",
       "        [-2.7333],\n",
       "        [-2.7313],\n",
       "        [-2.7218],\n",
       "        [-2.7315],\n",
       "        [-2.7290],\n",
       "        [-2.7316],\n",
       "        [-2.7246],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7257],\n",
       "        [-2.7328],\n",
       "        [-2.7349],\n",
       "        [-2.7220],\n",
       "        [-2.7311],\n",
       "        [-2.7227],\n",
       "        [-2.7199],\n",
       "        [-2.7304],\n",
       "        [-2.7326],\n",
       "        [-2.7388],\n",
       "        [-2.7322],\n",
       "        [-2.7233],\n",
       "        [-2.7243],\n",
       "        [-2.7230],\n",
       "        [-2.7367],\n",
       "        [-2.7298],\n",
       "        [-2.7324],\n",
       "        [-2.7333],\n",
       "        [-2.7226],\n",
       "        [-2.7281],\n",
       "        [-2.7237],\n",
       "        [-2.7322],\n",
       "        [-2.7293],\n",
       "        [-2.7231],\n",
       "        [-2.7207],\n",
       "        [-2.7364],\n",
       "        [-2.7344],\n",
       "        [-2.7308],\n",
       "        [-2.7364],\n",
       "        [-2.7323],\n",
       "        [-2.7260],\n",
       "        [-2.7262],\n",
       "        [-2.7301],\n",
       "        [-2.7209],\n",
       "        [-2.7212],\n",
       "        [-2.7315],\n",
       "        [-2.7257],\n",
       "        [-2.7381],\n",
       "        [-2.7270],\n",
       "        [-2.7191],\n",
       "        [-2.7275],\n",
       "        [-2.7317],\n",
       "        [-2.7330],\n",
       "        [-2.7282],\n",
       "        [-2.7277],\n",
       "        [-2.7319],\n",
       "        [-2.7232],\n",
       "        [-2.7245],\n",
       "        [-2.7219],\n",
       "        [-2.7298],\n",
       "        [-2.7199],\n",
       "        [-2.7256],\n",
       "        [-2.7252],\n",
       "        [-2.7296],\n",
       "        [-2.7335],\n",
       "        [-2.7267],\n",
       "        [-2.7314],\n",
       "        [-2.7328],\n",
       "        [-2.7273],\n",
       "        [-2.7308],\n",
       "        [-2.7331],\n",
       "        [-2.7353],\n",
       "        [-2.7304],\n",
       "        [-2.7176],\n",
       "        [-2.7261],\n",
       "        [-2.7284],\n",
       "        [-2.7234],\n",
       "        [-2.7323],\n",
       "        [-2.7328],\n",
       "        [-2.7371],\n",
       "        [-2.7287],\n",
       "        [-2.7245],\n",
       "        [-2.7244],\n",
       "        [-2.7270],\n",
       "        [-2.7324],\n",
       "        [-2.7260],\n",
       "        [-2.7295],\n",
       "        [-2.7261],\n",
       "        [-2.7358],\n",
       "        [-2.7261],\n",
       "        [-2.7282],\n",
       "        [-2.7257],\n",
       "        [-2.7248],\n",
       "        [-2.7390],\n",
       "        [-2.7267],\n",
       "        [-2.7387],\n",
       "        [-2.7252],\n",
       "        [-2.7323],\n",
       "        [-2.7375],\n",
       "        [-2.7289],\n",
       "        [-2.7218]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 1])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 300])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x) - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(layer(x).ravel() - y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 82.40032\n",
      "MSE на шаге 2 34.83059\n",
      "MSE на шаге 3 18.67639\n",
      "MSE на шаге 4 12.57786\n",
      "MSE на шаге 5 9.87140\n",
      "MSE на шаге 6 8.42349\n",
      "MSE на шаге 7 7.51441\n",
      "MSE на шаге 8 6.87739\n",
      "MSE на шаге 9 6.39865\n",
      "MSE на шаге 10 6.02125\n",
      "MSE на шаге 11 5.71254\n",
      "MSE на шаге 12 5.45201\n",
      "MSE на шаге 13 5.22616\n",
      "MSE на шаге 14 5.02584\n",
      "MSE на шаге 15 4.84476\n",
      "MSE на шаге 16 4.67857\n",
      "MSE на шаге 17 4.52421\n",
      "MSE на шаге 18 4.37955\n",
      "MSE на шаге 19 4.24304\n",
      "MSE на шаге 20 4.11358\n",
      "MSE на шаге 21 3.99035\n",
      "MSE на шаге 31 3.01138\n",
      "MSE на шаге 41 2.35799\n",
      "MSE на шаге 51 1.91931\n",
      "MSE на шаге 61 1.62472\n",
      "MSE на шаге 71 1.42689\n",
      "MSE на шаге 81 1.29405\n",
      "MSE на шаге 91 1.20484\n",
      "MSE на шаге 101 1.14493\n",
      "MSE на шаге 111 1.10470\n",
      "MSE на шаге 121 1.07768\n",
      "MSE на шаге 131 1.05954\n",
      "MSE на шаге 141 1.04736\n",
      "MSE на шаге 151 1.03917\n",
      "MSE на шаге 161 1.03368\n",
      "MSE на шаге 171 1.02999\n",
      "MSE на шаге 181 1.02751\n",
      "MSE на шаге 191 1.02585\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 10 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 5\n",
    "n_objects = 300\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "w_true = torch.randn(n_features)\n",
    "\n",
    "x = (torch.rand(n_objects, n_features) - 0.5) * 10 * (torch.arange(n_features) * 2 + 1)\n",
    "y = torch.matmul(x, w_true) + torch.randn(n_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1401.99646\n",
      "MSE на шаге 2 414.87131\n",
      "MSE на шаге 3 155.27328\n",
      "MSE на шаге 4 70.41111\n",
      "MSE на шаге 5 40.67516\n",
      "MSE на шаге 6 29.64690\n",
      "MSE на шаге 7 25.18774\n",
      "MSE на шаге 8 23.08298\n",
      "MSE на шаге 9 21.83970\n",
      "MSE на шаге 10 20.92380\n",
      "MSE на шаге 11 20.14287\n",
      "MSE на шаге 12 19.42708\n",
      "MSE на шаге 13 18.75065\n",
      "MSE на шаге 14 18.10368\n",
      "MSE на шаге 15 17.48201\n",
      "MSE на шаге 16 16.88361\n",
      "MSE на шаге 17 16.30720\n",
      "MSE на шаге 18 15.75182\n",
      "MSE на шаге 19 15.21665\n",
      "MSE на шаге 20 14.70092\n",
      "MSE на шаге 51 5.35484\n",
      "MSE на шаге 101 1.66924\n",
      "MSE на шаге 151 1.07477\n",
      "MSE на шаге 201 0.96975\n",
      "MSE на шаге 251 0.94388\n",
      "MSE на шаге 301 0.93214\n",
      "MSE на шаге 351 0.92400\n",
      "MSE на шаге 401 0.91755\n",
      "MSE на шаге 451 0.91229\n",
      "MSE на шаге 501 0.90798\n",
      "MSE на шаге 551 0.90444\n",
      "MSE на шаге 601 0.90153\n",
      "MSE на шаге 651 0.89914\n",
      "MSE на шаге 701 0.89718\n",
      "MSE на шаге 751 0.89557\n",
      "MSE на шаге 801 0.89425\n",
      "MSE на шаге 851 0.89316\n",
      "MSE на шаге 901 0.89227\n",
      "MSE на шаге 951 0.89154\n",
      "MSE на шаге 1001 0.89094\n",
      "MSE на шаге 1051 0.89045\n",
      "MSE на шаге 1101 0.89004\n",
      "MSE на шаге 1151 0.88971\n",
      "MSE на шаге 1201 0.88944\n",
      "MSE на шаге 1251 0.88921\n",
      "MSE на шаге 1301 0.88903\n",
      "MSE на шаге 1351 0.88888\n",
      "MSE на шаге 1401 0.88875\n",
      "MSE на шаге 1451 0.88865\n",
      "MSE на шаге 1501 0.88857\n",
      "MSE на шаге 1551 0.88850\n",
      "MSE на шаге 1601 0.88844\n",
      "MSE на шаге 1651 0.88839\n",
      "MSE на шаге 1701 0.88836\n",
      "MSE на шаге 1751 0.88832\n",
      "MSE на шаге 1801 0.88830\n",
      "MSE на шаге 1851 0.88828\n",
      "MSE на шаге 1901 0.88826\n",
      "MSE на шаге 1951 0.88825\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(in_features=n_features, out_features=1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer(x).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "    \n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer.weight -= layer.weight.grad * step_size\n",
    "        layer.bias -= layer.bias.grad * step_size\n",
    "\n",
    "    layer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = 2000\n",
    "step_size = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE на шаге 1 1987.39294\n",
      "MSE на шаге 2 1944.89038\n",
      "MSE на шаге 3 1907.61182\n",
      "MSE на шаге 4 1872.32373\n",
      "MSE на шаге 5 1839.18005\n",
      "MSE на шаге 6 1806.20276\n",
      "MSE на шаге 7 1773.40063\n",
      "MSE на шаге 8 1740.04749\n",
      "MSE на шаге 9 1706.15063\n",
      "MSE на шаге 10 1670.38672\n",
      "MSE на шаге 11 1631.73279\n",
      "MSE на шаге 12 1590.11755\n",
      "MSE на шаге 13 1545.43872\n",
      "MSE на шаге 14 1498.32947\n",
      "MSE на шаге 15 1449.01709\n",
      "MSE на шаге 16 1397.55151\n",
      "MSE на шаге 17 1344.03003\n",
      "MSE на шаге 18 1287.92188\n",
      "MSE на шаге 19 1230.70569\n",
      "MSE на шаге 20 1173.74146\n",
      "MSE на шаге 51 279.80460\n",
      "MSE на шаге 101 38.52470\n",
      "MSE на шаге 151 10.08229\n",
      "MSE на шаге 201 7.92840\n",
      "MSE на шаге 251 6.49304\n",
      "MSE на шаге 301 5.36825\n",
      "MSE на шаге 351 4.47649\n",
      "MSE на шаге 401 3.76643\n",
      "MSE на шаге 451 3.19856\n",
      "MSE на шаге 501 2.73992\n",
      "MSE на шаге 551 2.36663\n",
      "MSE на шаге 601 2.06977\n",
      "MSE на шаге 651 1.83486\n",
      "MSE на шаге 701 1.64795\n",
      "MSE на шаге 751 1.49623\n",
      "MSE на шаге 801 1.37047\n",
      "MSE на шаге 851 1.27038\n",
      "MSE на шаге 901 1.19571\n",
      "MSE на шаге 951 1.13851\n",
      "MSE на шаге 1001 1.09423\n",
      "MSE на шаге 1051 1.06086\n",
      "MSE на шаге 1101 1.03487\n",
      "MSE на шаге 1151 1.01447\n",
      "MSE на шаге 1201 0.99874\n",
      "MSE на шаге 1251 0.98668\n",
      "MSE на шаге 1301 0.97723\n",
      "MSE на шаге 1351 0.96980\n",
      "MSE на шаге 1401 0.96391\n",
      "MSE на шаге 1451 0.95921\n",
      "MSE на шаге 1501 0.95541\n",
      "MSE на шаге 1551 0.95231\n",
      "MSE на шаге 1601 0.94978\n",
      "MSE на шаге 1651 0.94771\n",
      "MSE на шаге 1701 0.94595\n",
      "MSE на шаге 1751 0.94443\n",
      "MSE на шаге 1801 0.94311\n",
      "MSE на шаге 1851 0.94192\n",
      "MSE на шаге 1901 0.94085\n",
      "MSE на шаге 1951 0.93987\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=n_features, out_features=3)\n",
    "layer2 = nn.Linear(in_features=3, out_features=1)\n",
    "activation = nn.ReLU()\n",
    "\n",
    "\n",
    "for i in range(n_steps):\n",
    "    y_pred = layer2(activation(layer1(x))).ravel()\n",
    "\n",
    "    mse = torch.mean((y_pred - y) ** 2)\n",
    "\n",
    "    if i < 20 or i % 50 == 0:\n",
    "        print(f'MSE на шаге {i + 1} {mse.item():.5f}')\n",
    "\n",
    "    mse.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        layer1.weight -= layer1.weight.grad * step_size\n",
    "        layer1.bias -= layer1.bias.grad * step_size\n",
    "        layer2.weight -= layer2.weight.grad * step_size\n",
    "        layer2.bias -= layer2.bias.grad * step_size\n",
    "\n",
    "    layer1.zero_grad()\n",
    "    layer2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar 1. Intro to DL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}