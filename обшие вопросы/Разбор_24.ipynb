{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ca41ce",
   "metadata": {},
   "source": [
    "### Блок теоретических вопросов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e2a439",
   "metadata": {},
   "source": [
    "*Как вы знаете, в SVM мы обучаем классификатор на hinge-loss, но при этом дополнительно добавляем L2 норму весов в наш функционал. Зачем?*: \n",
    "\n",
    "1.\tЭмпирически замечено, что переобученная модель имеет большую норму весов, поэтому мы хотим ее снизить в надежде на то, что тогда наша модель не будет переобученной\n",
    "2.\tДобавление нормы весов увеличивает шансы на то, что модель будет правильно разделять наши классы \n",
    "3.\tЭто непосредственно выводится из идеи о том, что мы бы хотели делать как можно меньше предположений о наших данных, поэтому проводим нашу гиперплоскость как можно дальше от всех объектов с учетом правильной (или почти правильной) классификации\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** при этом зачастую данные оказываются линейно неразделимы, как раз введение регуляризации позволяет решить этот вопрос.\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52edf8",
   "metadata": {},
   "source": [
    "*Пусть у нас имеется 2 положительных объекта и 3 отрицательных. Известно, что у построенного классификаторов accuracy < 1. Какое максимальное значение F-1 score может получиться?*: \n",
    "\n",
    "1.\t 0.57\n",
    "2.\t1\n",
    "3.\t0.8\n",
    "4.\t0.66\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** Пояснение: разбиение будет, например, такое + + - | - - \n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada404f6",
   "metadata": {},
   "source": [
    "*Рассмотрим модификацию алгоритма K-nn – Weighted K-nn. В чем его преимущество перед оригинальным алгоритмом?*: \n",
    "\n",
    "1.\tИспользование весов служит в качестве регуляризации\n",
    "2.\tВеса ограничивают вклад ближайших соседей в прогноз на новом объекте\n",
    "3.\tВеса учитывают расстояние до соседей, это помогает “расставлять приоритеты”, когда мы считаем прогноз на новом объекте\n",
    "4.\tВеса позволяют не брать в расчет некоторые из ближайших соседей, что значительно ускоряет процесс применения \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** Пояснение: некоторые соседи могут оказаться достаточно далеко от экземпляра, поданного модели. Тогда, кажется, достаточно логично учитывать их с мЕньшим весом, чем ближайших.\n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f04e6",
   "metadata": {},
   "source": [
    "*Рассмотрим два алгоритма – случайный лес и градиентный бустинг. В чем вы видите преимущество первого перед вторым в задаче регрессии?*: \n",
    "\n",
    "1.\tТак как в случайном лесе мы берем глубокие деревья, это позволяет получать качество выше, чем у бустинга. \n",
    "2.\t На самом деле особых преимуществ у случайного леса перед бустингом нет, поэтому этот алгоритм так редко применяется в реальных задачах\n",
    "3.\tДля леса можно особо не подбирать количество базовых деревьев, так как после определенного их количества ошибка становится близкой к константе, в то время как для бустинга это очень важный гиперпараметр\n",
    "4.\tКак известно, случайный лес не переобучается, а бустинг – вполне. Поэтому работа с лесом гораздо проще, в то время как для бустинга необходим файн тьюнинг, при том, что качество у них почти всегда примерно одинаковое\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** само собой лес можно переобучить, 4 пункт – типичное заблуждение.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debb381",
   "metadata": {},
   "source": [
    "*Как бы вы стали обучать стэкинг моделей и следить за качеством и почему?*: \n",
    "\n",
    "1.\tБазовые модели обучаем на обучающей выборке, применяем к валидационной выборке, на этих выходах обучаем нашу главную модель и качество смотрим на тестовой. Также можно использовать кросс-валидацию. Объяснение: наша итоговая модель должна понимать, как ошибаются базовые модели на новых данных (не переобучаемся под обучающие данные)\n",
    "2.\tБазовые модели обучаем на обучающей выборке, применяем к обучающей выборке, на этих выходах обучаем нашу главную модель и качество смотрим на отложенной. Также можно использовать кросс-валидацию. Объяснение: наша итоговая модель должна знать, как ошибаются базовые модели на обучающей выборке, чтобы не обучаться на выходы базовых на новых данных, так как они скорее всего будут далеки от правды.\n",
    "3.\tБазовые модели обучаем на обучающей выборке, применяем к валидационной выборке, на этих выходах обучаем нашу главную модель и качество смотрим на отложенной. Также можно использовать кросс-валидацию. Объяснение: наша итоговая модель должна знать, как ошибаются базовые модели на новых данных, так как их обычно значительно меньше, чем обучающих, а результат будет почти такой же.\n",
    "4.\tБазовые модели обучаем на обучающей выборке, применяем к обучающей выборке, на этих выходах обучаем нашу главную модель и качество смотрим на отложенной. Кросс-валидация для стекинга неприменима. Объяснение: наша итоговая модель должна понимать, как ошибаются базовые на новых данных, чтобы не подгоняться под обучающую выборку.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 1)** 2 очевидно неверный, так как мы в таком случае при применении к ошибкам базовых моделей добавим еще и ошибки нашей финальной модели. 3 неверный по понятным причинам. 4 неверный, так как аналог кросс-валидации применим к стекингу, этот вариант используется, если есть достаточно много времени на обучение модели.\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db119944",
   "metadata": {},
   "source": [
    "*Зачем в бэггинге деревьев используют именно независимые базовые алгоритмы и как это влияет на смещение и разброс всего ансамбля?*: \n",
    "\n",
    "1.\tТак как если усреднять ответы похожих моделей в бэггинге, то не удастся значительно снизить смещение финальной композиции из-за скоррелированности базовых моделей. \n",
    "2.\tТак как формально расписывая разброс бэггинга, получим, что он в N раз меньше разброса базовой модели, при зависимости деревьев, это свойство теряется.\n",
    "3.\tТак как при усреднении предсказаний каждого дерева мы избавляемся от шума в данных, что невозможно, если базовые модели зависимы.\n",
    "4.\tТак как ковариация между базовыми моделями входит в формулу смещения композиции, и если она ненулевая, то смещение всего ансамбля будет больше. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 2)** смещение бэггинга не меняется по сравнению с базовым алгоритмом, так как способность находить истинную зависимость у $N$ деревьев такая же, как и у одного. Однако при усреднении можно уменьшить эффект от разброса каждого отдельного дерева. \n",
    "В формальной формуле получится, что разброс $a(x) = \\frac{1}{N} * (разброс \\; b_i(x) + CoV(b_i(x), b_j(x)))$, где ковариация – это мера линейной зависимости $i$ и $j$ базовой модели. При большой ковариации (зависимости базовых алгоритмов) разброс выше, чем при околонулевой (независимости). В этом случае эффект от ансамбля деревьев снижается. \n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f5824",
   "metadata": {},
   "source": [
    "*Чем различается случайный лес и бэггинг деревьев?*: \n",
    "\n",
    "1.\tБэггинг деревьев не использует критерий информативности при обучении, тогда как лес минимизирует его при построении каждого отдельного дерева.\n",
    "2.\tБэггинг деревьев использует и бутстропированную выборку и метод случайных подпростанств, тогда как случайный лес только бутстропированную выборку.\n",
    "3.\tСлучайный лес, в отличие от бэггинга подбирает предикат, согласно критерию информативности не по всем признакам, а только по их подмножеству.\n",
    "4.\tСлучайный лес, в отличие от бэггинга обучает каждый базовый алгоритм не на изначальной выборке, а на бутстропированной, тем самым вводя случайность в модель.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** в обоих моделях используется бутстропированная выборка и критерий информативности при разделении листов, однако в случайном лесе базовые алгоритмы еще более независимы, так как каждое разбиение производится по случайному подмножеству признаков. \n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e105f",
   "metadata": {},
   "source": [
    "*Какая из следующих моделей может идеально разделить обучающую выборку в задаче бинарной классификации, если выборка не является линейно разделимой?*: \n",
    "\n",
    "1.\tkNN с одним соседом.\n",
    "2.\tSVM с изотонической регрессией над выходами.\n",
    "3.\tЛогистическая регрессия с l2-регуляризацией.\n",
    "4.\tДостаточно глубокое решающее дерево с неединичным минимальным количеством объектов в листе. \n",
    "5.\tГрадиентный бустинг над lasso-регрессиями. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 1)** kNN запоминает обучающую выборку, тогда при предсказании на обучающей выборке единственный ближайший сосед это тот же объект с тем же классом. В стандартном SVM гиперплоскость не может разделить линейно неразделимую выборку, если нет преобразований над признаками (полиномиальные признаки, ядровой SVM). Логистическая регрессия – это также линейный метод. Решающее дерево может запомнить выборку, при отсутствии ограничения на глубину и единичным количеством объектов в листе. Градиентный бустинг над линейными моделями – это тоже линейная модель. \n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c63fa",
   "metadata": {},
   "source": [
    "*Допустим, мы решаем задачу многоклассовой классификации, где в качестве целевой переменной выступают времена года. Как решить эту задачу, используя бинарный классификатор и сколько моделей необходимо обучить?*: \n",
    "\n",
    "1.\tМожно обучить 4 one-vs-rest бинарных классификатора и выбрать класс той модели, у которой наименьшая вероятность.\n",
    "2.\tМожно обучить 3 one-vs-rest бинарных классификатора и выбрать класс той модели, у которой наибольшая вероятность.\n",
    "3.\tМожно обучить 6 all-vs-all бинарных классификатора и выбрать класс голосованием большинством.\n",
    "4.\tМожно обучить 5 all-vs-all бинарных классификатора и выбрать класс голосованием большинством.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** в one-vs-rest подходе требуется обучить 4 модели (зима-vs-остальные; лето-vs-остальные и т.д.) и выбрать тот класс, где модель наиболее уверена. Тогда как в all-vs-all подходе нужно попарно обучить модели на всех метках (зима-vs-лето, осень, весна; лето-vs-осень, весна; осень-vs-весна), итого получится 6 моделей и класс выбирается простым голосованием большинства. Математически количество бинарных моделей можно посчитать как число сочетаний из n по 2, где n – количество классов. \n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c68d7a",
   "metadata": {},
   "source": [
    "*Какой формы получаются кластеры в методе k-means и DBSCAN, если в качестве метрики расстояния использовать метрику Минковского с p = 1?*: \n",
    "\n",
    "1.\tКластеры будут ромбовидными в методе k-means, и шарообразными в методе DBSCAN.\n",
    "2.\tКластеры будут шарообразными в методе k-means, и любой формы в методе DBSCAN.\n",
    "3.\tКластеры будут ромбовидными в методе k-means, и любой формы в методе DBSCAN.\n",
    "4.\tКластеры будут шарообразными в методе k-means, и ромбовидными в методе DBSCAN.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 3)** Пояснение: метрика Минковского с p = 1 – это манхэттенское расстояние, где объекты на одинаковом расстоянии находятся на ромбе, тогда как в евклидовой метрике – на окружности.\n",
    "K-means минимизирует внутрикластерное расстояние, которое высчитывается по формуле манхэттенского расстояния, тогда как DBSCAN использует только eps-окрестность для классификации точки как ядровая/пограничная/шумовая. \n",
    "\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2652c5f",
   "metadata": {},
   "source": [
    "*Чем отличаются задачи кластеризации и классификации?*: \n",
    "\n",
    "1.\tВ задаче классификации есть и обучающая и тестовая выборка, тогда как в кластеризации у нас имеется только обучающая.\n",
    "2.\tВ задаче кластеризации надо по целевой переменной разбить объекты на группы, исходя из предположений об устройстве данных, тогда как в классификации требуется разделить объекты по их признакам.\n",
    "3.\tВ задаче кластеризации у объектов нет признакового описания и его как раз необходимо построить, минимизируя внутрикластерное расстояние.\n",
    "4.\tВ задаче классификации по объектам и их меткам нужно построить модель, тогда как в кластеризации по признаковому описанию объектов нужно присвоить им метки.\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Ответ: 4)** в задаче кластеризации в отличие от классификации нет целевой переменной, по которой строится модель. Модели кластеризации относятся к unsupervised методам (обучение без учителя), здесь мы присваиваем объектам метки одного класса, в предположении о количестве кластеров, как в методе k-means или предполагая, что они достаточно плотно расположены, как в методе DBSCAN.  \n",
    "\n",
    "___________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
